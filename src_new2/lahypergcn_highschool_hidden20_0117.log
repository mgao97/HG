args
 Namespace(samples=4, concat=4, runs=3, latent_size=10, dataset='highschool', seed=42, epochs=500, lr=0.01, weight_decay=0.0005, hidden=20, dropout=0.5, batch_size=128, tem=0.5, lam=1.0, pretrain_epochs=15, pretrain_lr=0.05, conditional=True, update_epochs=20, num_models=100, warmup=200, use_mediator=False, cuda=False)
X dim: torch.Size([327, 327])
labels: 9
Run Train:   0%|          | 0/3 [00:00<?, ?it/s]/users/Min/HG/src_new2/lahypergcn_highschool_hidden20.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  cvae_features = torch.tensor(X, dtype=torch.float32).to(device)
/users/Min/miniconda/envs/hy/lib/python3.9/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
model: LAHyperGCN(
  (hygcn1): HyperGCNConv(
    (act): ReLU(inplace=True)
    (drop): Dropout(p=0.5, inplace=False)
    (theta): Linear(in_features=327, out_features=20, bias=True)
  )
  (hygcn2): HyperGCNConv(
    (act): ReLU(inplace=True)
    (drop): Dropout(p=0.5, inplace=False)
    (theta): Linear(in_features=100, out_features=9, bias=True)
  )
)
Run:01 Epoch: 0001 loss_train: 2.2031 loss_val: 2.1947
Run:01 Epoch: 0002 loss_train: 2.1981 loss_val: 2.1918
Run:01 Epoch: 0003 loss_train: 2.1928 loss_val: 2.1878
Run:01 Epoch: 0004 loss_train: 2.1870 loss_val: 2.1835
Run:01 Epoch: 0005 loss_train: 2.1794 loss_val: 2.1788
Run:01 Epoch: 0006 loss_train: 2.1716 loss_val: 2.1747
Run:01 Epoch: 0007 loss_train: 2.1624 loss_val: 2.1703
Run:01 Epoch: 0008 loss_train: 2.1532 loss_val: 2.1650
Run:01 Epoch: 0009 loss_train: 2.1436 loss_val: 2.1601
Run:01 Epoch: 0010 loss_train: 2.1304 loss_val: 2.1535
Run:01 Epoch: 0011 loss_train: 2.1220 loss_val: 2.1462
Run:01 Epoch: 0012 loss_train: 2.1074 loss_val: 2.1350
Run:01 Epoch: 0013 loss_train: 2.0943 loss_val: 2.1215
Run:01 Epoch: 0014 loss_train: 2.0784 loss_val: 2.1044
Run:01 Epoch: 0015 loss_train: 2.0610 loss_val: 2.0827
Run:01 Epoch: 0016 loss_train: 2.0373 loss_val: 2.0607
Run:01 Epoch: 0017 loss_train: 2.0171 loss_val: 2.0321
Run:01 Epoch: 0018 loss_train: 1.9922 loss_val: 2.0011
Run:01 Epoch: 0019 loss_train: 1.9676 loss_val: 1.9704
Run:01 Epoch: 0020 loss_train: 1.9414 loss_val: 1.9336
Run:01 Epoch: 0021 loss_train: 1.8991 loss_val: 1.8981
Run:01 Epoch: 0022 loss_train: 1.8735 loss_val: 1.8572
Run:01 Epoch: 0023 loss_train: 1.8283 loss_val: 1.8118
Run:01 Epoch: 0024 loss_train: 1.7910 loss_val: 1.7660
Run:01 Epoch: 0025 loss_train: 1.7540 loss_val: 1.7165
Run:01 Epoch: 0026 loss_train: 1.7178 loss_val: 1.6721
Run:01 Epoch: 0027 loss_train: 1.6736 loss_val: 1.6211
Run:01 Epoch: 0028 loss_train: 1.6277 loss_val: 1.5735
Run:01 Epoch: 0029 loss_train: 1.5981 loss_val: 1.5230
Run:01 Epoch: 0030 loss_train: 1.5525 loss_val: 1.4706
Run:01 Epoch: 0031 loss_train: 1.4947 loss_val: 1.4247
Run:01 Epoch: 0032 loss_train: 1.4580 loss_val: 1.3690
Run:01 Epoch: 0033 loss_train: 1.4159 loss_val: 1.3323
Run:01 Epoch: 0034 loss_train: 1.3657 loss_val: 1.2736
Run:01 Epoch: 0035 loss_train: 1.3216 loss_val: 1.2331
Run:01 Epoch: 0036 loss_train: 1.2770 loss_val: 1.1786
Run:01 Epoch: 0037 loss_train: 1.2247 loss_val: 1.1217
Run:01 Epoch: 0038 loss_train: 1.1639 loss_val: 1.0693
Run:01 Epoch: 0039 loss_train: 1.1261 loss_val: 1.0170
Run:01 Epoch: 0040 loss_train: 1.0932 loss_val: 0.9718
Run:01 Epoch: 0041 loss_train: 1.0455 loss_val: 0.9279
Run:01 Epoch: 0042 loss_train: 1.0094 loss_val: 0.8853
Run:01 Epoch: 0043 loss_train: 0.9614 loss_val: 0.8461
Run:01 Epoch: 0044 loss_train: 0.9184 loss_val: 0.8034
Run:01 Epoch: 0045 loss_train: 0.8591 loss_val: 0.7628
Run:01 Epoch: 0046 loss_train: 0.8353 loss_val: 0.7198
Run:01 Epoch: 0047 loss_train: 0.8058 loss_val: 0.6843
Run:01 Epoch: 0048 loss_train: 0.7453 loss_val: 0.6493
Run:01 Epoch: 0049 loss_train: 0.7256 loss_val: 0.6230
Run:01 Epoch: 0050 loss_train: 0.6866 loss_val: 0.5906
Run:01 Epoch: 0051 loss_train: 0.6743 loss_val: 0.5660
Run:01 Epoch: 0052 loss_train: 0.6196 loss_val: 0.5393
Run:01 Epoch: 0053 loss_train: 0.6131 loss_val: 0.5097
Run:01 Epoch: 0054 loss_train: 0.5755 loss_val: 0.4819
Run:01 Epoch: 0055 loss_train: 0.5402 loss_val: 0.4628
Run:01 Epoch: 0056 loss_train: 0.5135 loss_val: 0.4468
Run:01 Epoch: 0057 loss_train: 0.5149 loss_val: 0.4313
Run:01 Epoch: 0058 loss_train: 0.4821 loss_val: 0.4087
Run:01 Epoch: 0059 loss_train: 0.4569 loss_val: 0.3913
Run:01 Epoch: 0060 loss_train: 0.4419 loss_val: 0.3737
Run:01 Epoch: 0061 loss_train: 0.4186 loss_val: 0.3574
Run:01 Epoch: 0062 loss_train: 0.4172 loss_val: 0.3382
Run:01 Epoch: 0063 loss_train: 0.3847 loss_val: 0.3220
Run:01 Epoch: 0064 loss_train: 0.3773 loss_val: 0.3124
Run:01 Epoch: 0065 loss_train: 0.3610 loss_val: 0.2994
Run:01 Epoch: 0066 loss_train: 0.3645 loss_val: 0.2854
Run:01 Epoch: 0067 loss_train: 0.3400 loss_val: 0.2812
Run:01 Epoch: 0068 loss_train: 0.3379 loss_val: 0.2834
Run:01 Epoch: 0069 loss_train: 0.3279 loss_val: 0.2700
Run:01 Epoch: 0070 loss_train: 0.3251 loss_val: 0.2639
Run:01 Epoch: 0071 loss_train: 0.3017 loss_val: 0.2518
Run:01 Epoch: 0072 loss_train: 0.2901 loss_val: 0.2387
Run:01 Epoch: 0073 loss_train: 0.3094 loss_val: 0.2310
Run:01 Epoch: 0074 loss_train: 0.2817 loss_val: 0.2179
Run:01 Epoch: 0075 loss_train: 0.2837 loss_val: 0.2198
Run:01 Epoch: 0076 loss_train: 0.3024 loss_val: 0.2196
Run:01 Epoch: 0077 loss_train: 0.2772 loss_val: 0.2239
Run:01 Epoch: 0078 loss_train: 0.2724 loss_val: 0.2174
Run:01 Epoch: 0079 loss_train: 0.2593 loss_val: 0.2055
Run:01 Epoch: 0080 loss_train: 0.2580 loss_val: 0.1999
Run:01 Epoch: 0081 loss_train: 0.2611 loss_val: 0.1909
Run:01 Epoch: 0082 loss_train: 0.2495 loss_val: 0.1876
Run:01 Epoch: 0083 loss_train: 0.2520 loss_val: 0.1825
Run:01 Epoch: 0084 loss_train: 0.2206 loss_val: 0.1824
Run:01 Epoch: 0085 loss_train: 0.2398 loss_val: 0.1820
Run:01 Epoch: 0086 loss_train: 0.2321 loss_val: 0.1838
Run:01 Epoch: 0087 loss_train: 0.2391 loss_val: 0.1811
Run:01 Epoch: 0088 loss_train: 0.2360 loss_val: 0.1785
Run:01 Epoch: 0089 loss_train: 0.2290 loss_val: 0.1753
Run:01 Epoch: 0090 loss_train: 0.2336 loss_val: 0.1725
Run:01 Epoch: 0091 loss_train: 0.2572 loss_val: 0.1686
Run:01 Epoch: 0092 loss_train: 0.2214 loss_val: 0.1604
Run:01 Epoch: 0093 loss_train: 0.2292 loss_val: 0.1614
Run:01 Epoch: 0094 loss_train: 0.2416 loss_val: 0.1631
Run:01 Epoch: 0095 loss_train: 0.2392 loss_val: 0.1637
Run:01 Epoch: 0096 loss_train: 0.2181 loss_val: 0.1685
Run:01 Epoch: 0097 loss_train: 0.2282 loss_val: 0.1655
Run:01 Epoch: 0098 loss_train: 0.2165 loss_val: 0.1622
Run:01 Epoch: 0099 loss_train: 0.2184 loss_val: 0.1564
Run:01 Epoch: 0100 loss_train: 0.1955 loss_val: 0.1501
Run:01 Epoch: 0101 loss_train: 0.2024 loss_val: 0.1470
Run:01 Epoch: 0102 loss_train: 0.2284 loss_val: 0.1440
Run:01 Epoch: 0103 loss_train: 0.2105 loss_val: 0.1448
Run:01 Epoch: 0104 loss_train: 0.2097 loss_val: 0.1457
Run:01 Epoch: 0105 loss_train: 0.2014 loss_val: 0.1453
Run:01 Epoch: 0106 loss_train: 0.1988 loss_val: 0.1454
Run:01 Epoch: 0107 loss_train: 0.1941 loss_val: 0.1502
Run:01 Epoch: 0108 loss_train: 0.1851 loss_val: 0.1441
Run:01 Epoch: 0109 loss_train: 0.1933 loss_val: 0.1331
Run:01 Epoch: 0110 loss_train: 0.1878 loss_val: 0.1353
Run:01 Epoch: 0111 loss_train: 0.1938 loss_val: 0.1353
Run:01 Epoch: 0112 loss_train: 0.1899 loss_val: 0.1319
Run:01 Epoch: 0113 loss_train: 0.1988 loss_val: 0.1308
Run:01 Epoch: 0114 loss_train: 0.2065 loss_val: 0.1301
Run:01 Epoch: 0115 loss_train: 0.1999 loss_val: 0.1317
Run:01 Epoch: 0116 loss_train: 0.1950 loss_val: 0.1319
Run:01 Epoch: 0117 loss_train: 0.1815 loss_val: 0.1346
Run:01 Epoch: 0118 loss_train: 0.1882 loss_val: 0.1316
Run:01 Epoch: 0119 loss_train: 0.1764 loss_val: 0.1307
Run:01 Epoch: 0120 loss_train: 0.1647 loss_val: 0.1289
Run:01 Epoch: 0121 loss_train: 0.1751 loss_val: 0.1238
Run:01 Epoch: 0122 loss_train: 0.1893 loss_val: 0.1189
Run:01 Epoch: 0123 loss_train: 0.1821 loss_val: 0.1232
Run:01 Epoch: 0124 loss_train: 0.1678 loss_val: 0.1155
Run:01 Epoch: 0125 loss_train: 0.1684 loss_val: 0.1151
Run:01 Epoch: 0126 loss_train: 0.1732 loss_val: 0.1188
Run:01 Epoch: 0127 loss_train: 0.1733 loss_val: 0.1195
Run:01 Epoch: 0128 loss_train: 0.1650 loss_val: 0.1206
Run:01 Epoch: 0129 loss_train: 0.1749 loss_val: 0.1193
Run:01 Epoch: 0130 loss_train: 0.1628 loss_val: 0.1227
Run:01 Epoch: 0131 loss_train: 0.1728 loss_val: 0.1182
Run:01 Epoch: 0132 loss_train: 0.1578 loss_val: 0.1190
Run:01 Epoch: 0133 loss_train: 0.1515 loss_val: 0.1155
Run:01 Epoch: 0134 loss_train: 0.1627 loss_val: 0.1098
Run:01 Epoch: 0135 loss_train: 0.1747 loss_val: 0.1117
Run:01 Epoch: 0136 loss_train: 0.1622 loss_val: 0.1124
Run:01 Epoch: 0137 loss_train: 0.1651 loss_val: 0.1098
Run:01 Epoch: 0138 loss_train: 0.1587 loss_val: 0.1092
Run:01 Epoch: 0139 loss_train: 0.1668 loss_val: 0.1110
Run:01 Epoch: 0140 loss_train: 0.1577 loss_val: 0.1075
Run:01 Epoch: 0141 loss_train: 0.1527 loss_val: 0.1141
Run:01 Epoch: 0142 loss_train: 0.1417 loss_val: 0.1117
Run:01 Epoch: 0143 loss_train: 0.1435 loss_val: 0.1134
Run:01 Epoch: 0144 loss_train: 0.1525 loss_val: 0.1130
Run:01 Epoch: 0145 loss_train: 0.1519 loss_val: 0.1122
Run:01 Epoch: 0146 loss_train: 0.1562 loss_val: 0.1104
Run:01 Epoch: 0147 loss_train: 0.1525 loss_val: 0.1090
Run:01 Epoch: 0148 loss_train: 0.1504 loss_val: 0.1038
Run:01 Epoch: 0149 loss_train: 0.1521 loss_val: 0.1031
Run:01 Epoch: 0150 loss_train: 0.1507 loss_val: 0.1021
Run:01 Epoch: 0151 loss_train: 0.1552 loss_val: 0.1019
Run:01 Epoch: 0152 loss_train: 0.1446 loss_val: 0.1019
Run:01 Epoch: 0153 loss_train: 0.1553 loss_val: 0.1076
Run:01 Epoch: 0154 loss_train: 0.1491 loss_val: 0.1021
Run:01 Epoch: 0155 loss_train: 0.1473 loss_val: 0.0984
Run:01 Epoch: 0156 loss_train: 0.1581 loss_val: 0.1008
Run:01 Epoch: 0157 loss_train: 0.1541 loss_val: 0.1043
Run:01 Epoch: 0158 loss_train: 0.1500 loss_val: 0.1044
Run:01 Epoch: 0159 loss_train: 0.1561 loss_val: 0.1029
Run:01 Epoch: 0160 loss_train: 0.1556 loss_val: 0.1002
Run:01 Epoch: 0161 loss_train: 0.1490 loss_val: 0.1026
Run:01 Epoch: 0162 loss_train: 0.1338 loss_val: 0.0946
Run:01 Epoch: 0163 loss_train: 0.1338 loss_val: 0.0986
Run:01 Epoch: 0164 loss_train: 0.1460 loss_val: 0.0924
Run:01 Epoch: 0165 loss_train: 0.1424 loss_val: 0.0954
Run:01 Epoch: 0166 loss_train: 0.1359 loss_val: 0.0961
Run:01 Epoch: 0167 loss_train: 0.1310 loss_val: 0.0937
Run:01 Epoch: 0168 loss_train: 0.1319 loss_val: 0.0947
Run:01 Epoch: 0169 loss_train: 0.1417 loss_val: 0.0957
Run:01 Epoch: 0170 loss_train: 0.1426 loss_val: 0.1018
Run:01 Epoch: 0171 loss_train: 0.1430 loss_val: 0.0981
Run:01 Epoch: 0172 loss_train: 0.1285 loss_val: 0.0981
Run:01 Epoch: 0173 loss_train: 0.1391 loss_val: 0.0928
Run:01 Epoch: 0174 loss_train: 0.1429 loss_val: 0.0965
Run:01 Epoch: 0175 loss_train: 0.1360 loss_val: 0.0970
Run:01 Epoch: 0176 loss_train: 0.1321 loss_val: 0.0938
Run:01 Epoch: 0177 loss_train: 0.1403 loss_val: 0.0890
Run:01 Epoch: 0178 loss_train: 0.1388 loss_val: 0.0905
Run:01 Epoch: 0179 loss_train: 0.1313 loss_val: 0.0905
Run:01 Epoch: 0180 loss_train: 0.1384 loss_val: 0.0904
Run:01 Epoch: 0181 loss_train: 0.1315 loss_val: 0.0928
Run:01 Epoch: 0182 loss_train: 0.1403 loss_val: 0.0900
Run:01 Epoch: 0183 loss_train: 0.1234 loss_val: 0.0901
Run:01 Epoch: 0184 loss_train: 0.1292 loss_val: 0.0905
Run:01 Epoch: 0185 loss_train: 0.1262 loss_val: 0.0912
Run:01 Epoch: 0186 loss_train: 0.1315 loss_val: 0.0892
Run:01 Epoch: 0187 loss_train: 0.1257 loss_val: 0.0883
Run:01 Epoch: 0188 loss_train: 0.1267 loss_val: 0.0891
Run:01 Epoch: 0189 loss_train: 0.1232 loss_val: 0.0896
Run:01 Epoch: 0190 loss_train: 0.1280 loss_val: 0.0895
Run:01 Epoch: 0191 loss_train: 0.1313 loss_val: 0.0876
Run:01 Epoch: 0192 loss_train: 0.1296 loss_val: 0.0840
Run:01 Epoch: 0193 loss_train: 0.1355 loss_val: 0.0876
Run:01 Epoch: 0194 loss_train: 0.1175 loss_val: 0.0867
Run:01 Epoch: 0195 loss_train: 0.1353 loss_val: 0.0887
Run:01 Epoch: 0196 loss_train: 0.1192 loss_val: 0.0850
Run:01 Epoch: 0197 loss_train: 0.1221 loss_val: 0.0853
Run:01 Epoch: 0198 loss_train: 0.1179 loss_val: 0.0862
Run:01 Epoch: 0199 loss_train: 0.1223 loss_val: 0.0853
Run:01 Epoch: 0200 loss_train: 0.1230 loss_val: 0.0825
Run:01 Epoch: 0201 loss_train: 0.1326 loss_val: 0.0851
Run:01 Epoch: 0202 loss_train: 0.1398 loss_val: 0.0853
Run:01 Epoch: 0203 loss_train: 0.1163 loss_val: 0.0809
Run:01 Epoch: 0204 loss_train: 0.1156 loss_val: 0.0830
Run:01 Epoch: 0205 loss_train: 0.1317 loss_val: 0.0837
Run:01 Epoch: 0206 loss_train: 0.1185 loss_val: 0.0816
Run:01 Epoch: 0207 loss_train: 0.1258 loss_val: 0.0835
Run:01 Epoch: 0208 loss_train: 0.1261 loss_val: 0.0809
Run:01 Epoch: 0209 loss_train: 0.1280 loss_val: 0.0828
Run:01 Epoch: 0210 loss_train: 0.1196 loss_val: 0.0835
Run:01 Epoch: 0211 loss_train: 0.1235 loss_val: 0.0830
Run:01 Epoch: 0212 loss_train: 0.1213 loss_val: 0.0811
Run:01 Epoch: 0213 loss_train: 0.1212 loss_val: 0.0825
Run:01 Epoch: 0214 loss_train: 0.1154 loss_val: 0.0773
Run:01 Epoch: 0215 loss_train: 0.1238 loss_val: 0.0800
Run:01 Epoch: 0216 loss_train: 0.1155 loss_val: 0.0803
Run:01 Epoch: 0217 loss_train: 0.1160 loss_val: 0.0822
Run:01 Epoch: 0218 loss_train: 0.1186 loss_val: 0.0810
Run:01 Epoch: 0219 loss_train: 0.1090 loss_val: 0.0821
Run:01 Epoch: 0220 loss_train: 0.1072 loss_val: 0.0832
Run:01 Epoch: 0221 loss_train: 0.1205 loss_val: 0.0752
Run:01 Epoch: 0222 loss_train: 0.1206 loss_val: 0.0793
Run:01 Epoch: 0223 loss_train: 0.1149 loss_val: 0.0781
Run:01 Epoch: 0224 loss_train: 0.1079 loss_val: 0.0782
Run:01 Epoch: 0225 loss_train: 0.1158 loss_val: 0.0782
Run:01 Epoch: 0226 loss_train: 0.1270 loss_val: 0.0799
Run:01 Epoch: 0227 loss_train: 0.1104 loss_val: 0.0812
Run:01 Epoch: 0228 loss_train: 0.1225 loss_val: 0.0791
Run:01 Epoch: 0229 loss_train: 0.1129 loss_val: 0.0814
Run:01 Epoch: 0230 loss_train: 0.1264 loss_val: 0.0793
Run:01 Epoch: 0231 loss_train: 0.1105 loss_val: 0.0832
Run:01 Epoch: 0232 loss_train: 0.1198 loss_val: 0.0803
Run:01 Epoch: 0233 loss_train: 0.1186 loss_val: 0.0805
Run:01 Epoch: 0234 loss_train: 0.1007 loss_val: 0.0798
Run:01 Epoch: 0235 loss_train: 0.1157 loss_val: 0.0798
Run:01 Epoch: 0236 loss_train: 0.1209 loss_val: 0.0779
Run:01 Epoch: 0237 loss_train: 0.1199 loss_val: 0.0776
Run:01 Epoch: 0238 loss_train: 0.1188 loss_val: 0.0770
Run:01 Epoch: 0239 loss_train: 0.1109 loss_val: 0.0744
Run:01 Epoch: 0240 loss_train: 0.1114 loss_val: 0.0766
Run:01 Epoch: 0241 loss_train: 0.1135 loss_val: 0.0772
Run:01 Epoch: 0242 loss_train: 0.1115 loss_val: 0.0758
Run:01 Epoch: 0243 loss_train: 0.1173 loss_val: 0.0751
Run:01 Epoch: 0244 loss_train: 0.1181 loss_val: 0.0816
Run:01 Epoch: 0245 loss_train: 0.1099 loss_val: 0.0773
Run:01 Epoch: 0246 loss_train: 0.1145 loss_val: 0.0775
Run:01 Epoch: 0247 loss_train: 0.1095 loss_val: 0.0740
Run:01 Epoch: 0248 loss_train: 0.1095 loss_val: 0.0743
Run:01 Epoch: 0249 loss_train: 0.1079 loss_val: 0.0729
Run:01 Epoch: 0250 loss_train: 0.1155 loss_val: 0.0753
Run:01 Epoch: 0251 loss_train: 0.1154 loss_val: 0.0728
Run:01 Epoch: 0252 loss_train: 0.1080 loss_val: 0.0734
Run:01 Epoch: 0253 loss_train: 0.1134 loss_val: 0.0705
Run:01 Epoch: 0254 loss_train: 0.1165 loss_val: 0.0737
Run:01 Epoch: 0255 loss_train: 0.1013 loss_val: 0.0715
Run:01 Epoch: 0256 loss_train: 0.1020 loss_val: 0.0754
Run:01 Epoch: 0257 loss_train: 0.1026 loss_val: 0.0768
Run:01 Epoch: 0258 loss_train: 0.1109 loss_val: 0.0768
Run:01 Epoch: 0259 loss_train: 0.1130 loss_val: 0.0760
Run:01 Epoch: 0260 loss_train: 0.1152 loss_val: 0.0768
Run:01 Epoch: 0261 loss_train: 0.1003 loss_val: 0.0766
Run:01 Epoch: 0262 loss_train: 0.1137 loss_val: 0.0750
Run:01 Epoch: 0263 loss_train: 0.1221 loss_val: 0.0741
Run:01 Epoch: 0264 loss_train: 0.1075 loss_val: 0.0731
Run:01 Epoch: 0265 loss_train: 0.1056 loss_val: 0.0738
Run:01 Epoch: 0266 loss_train: 0.0960 loss_val: 0.0708
Run:01 Epoch: 0267 loss_train: 0.1104 loss_val: 0.0710
Run:01 Epoch: 0268 loss_train: 0.1086 loss_val: 0.0723
Run:01 Epoch: 0269 loss_train: 0.1141 loss_val: 0.0719
Run:01 Epoch: 0270 loss_train: 0.1080 loss_val: 0.0759
Run:01 Epoch: 0271 loss_train: 0.1031 loss_val: 0.0714
Run:01 Epoch: 0272 loss_train: 0.0997 loss_val: 0.0725
Run:01 Epoch: 0273 loss_train: 0.1025 loss_val: 0.0719
Run:01 Epoch: 0274 loss_train: 0.1085 loss_val: 0.0743
Run:01 Epoch: 0275 loss_train: 0.1050 loss_val: 0.0736
Run:01 Epoch: 0276 loss_train: 0.1024 loss_val: 0.0722
Run:01 Epoch: 0277 loss_train: 0.1074 loss_val: 0.0709
Run:01 Epoch: 0278 loss_train: 0.1011 loss_val: 0.0711
Run:01 Epoch: 0279 loss_train: 0.1002 loss_val: 0.0730
Run:01 Epoch: 0280 loss_train: 0.1103 loss_val: 0.0708
Run:01 Epoch: 0281 loss_train: 0.1121 loss_val: 0.0714
Run:01 Epoch: 0282 loss_train: 0.1086 loss_val: 0.0687
Run:01 Epoch: 0283 loss_train: 0.1143 loss_val: 0.0745
Run:01 Epoch: 0284 loss_train: 0.1036 loss_val: 0.0705
Run:01 Epoch: 0285 loss_train: 0.1025 loss_val: 0.0732
Run:01 Epoch: 0286 loss_train: 0.1137 loss_val: 0.0732
Run:01 Epoch: 0287 loss_train: 0.1096 loss_val: 0.0730
Run:01 Epoch: 0288 loss_train: 0.1010 loss_val: 0.0758
Run:01 Epoch: 0289 loss_train: 0.1021 loss_val: 0.0738
Run:01 Epoch: 0290 loss_train: 0.1105 loss_val: 0.0704
Run:01 Epoch: 0291 loss_train: 0.0989 loss_val: 0.0681
Run:01 Epoch: 0292 loss_train: 0.1043 loss_val: 0.0695
Run:01 Epoch: 0293 loss_train: 0.0992 loss_val: 0.0673
Run:01 Epoch: 0294 loss_train: 0.0989 loss_val: 0.0680
Run:01 Epoch: 0295 loss_train: 0.1016 loss_val: 0.0671
Run:01 Epoch: 0296 loss_train: 0.1013 loss_val: 0.0648
Run:01 Epoch: 0297 loss_train: 0.1021 loss_val: 0.0692
Run:01 Epoch: 0298 loss_train: 0.1090 loss_val: 0.0680
Run:01 Epoch: 0299 loss_train: 0.1000 loss_val: 0.0658
Run:01 Epoch: 0300 loss_train: 0.1091 loss_val: 0.0701
Run:01 Epoch: 0301 loss_train: 0.1033 loss_val: 0.0729
Run:01 Epoch: 0302 loss_train: 0.1000 loss_val: 0.0743
Run:01 Epoch: 0303 loss_train: 0.1050 loss_val: 0.0719
Run:01 Epoch: 0304 loss_train: 0.0970 loss_val: 0.0701
Run:01 Epoch: 0305 loss_train: 0.1008 loss_val: 0.0702
Run:01 Epoch: 0306 loss_train: 0.1018 loss_val: 0.0666
Run:01 Epoch: 0307 loss_train: 0.0905 loss_val: 0.0664
Run:01 Epoch: 0308 loss_train: 0.0957 loss_val: 0.0638
Run:01 Epoch: 0309 loss_train: 0.1007 loss_val: 0.0651
Run:01 Epoch: 0310 loss_train: 0.0924 loss_val: 0.0637
Run:01 Epoch: 0311 loss_train: 0.1020 loss_val: 0.0670
Run:01 Epoch: 0312 loss_train: 0.1002 loss_val: 0.0653
Run:01 Epoch: 0313 loss_train: 0.1036 loss_val: 0.0654
Run:01 Epoch: 0314 loss_train: 0.0996 loss_val: 0.0658
Run:01 Epoch: 0315 loss_train: 0.0902 loss_val: 0.0681
Run:01 Epoch: 0316 loss_train: 0.1077 loss_val: 0.0700
Run:01 Epoch: 0317 loss_train: 0.1085 loss_val: 0.0686
Run:01 Epoch: 0318 loss_train: 0.1009 loss_val: 0.0679
Run:01 Epoch: 0319 loss_train: 0.0985 loss_val: 0.0691
Run:01 Epoch: 0320 loss_train: 0.0971 loss_val: 0.0674
Run:01 Epoch: 0321 loss_train: 0.0912 loss_val: 0.0662
Run:01 Epoch: 0322 loss_train: 0.0920 loss_val: 0.0688
Run:01 Epoch: 0323 loss_train: 0.0931 loss_val: 0.0664
Run:01 Epoch: 0324 loss_train: 0.0964 loss_val: 0.0651
Run:01 Epoch: 0325 loss_train: 0.1033 loss_val: 0.0690
Run:01 Epoch: 0326 loss_train: 0.0984 loss_val: 0.0673
Run:01 Epoch: 0327 loss_train: 0.0955 loss_val: 0.0679
Run:01 Epoch: 0328 loss_train: 0.0991 loss_val: 0.0654
Run:01 Epoch: 0329 loss_train: 0.1042 loss_val: 0.0680
Run:01 Epoch: 0330 loss_train: 0.0950 loss_val: 0.0678
Run:01 Epoch: 0331 loss_train: 0.1030 loss_val: 0.0672
Run:01 Epoch: 0332 loss_train: 0.0967 loss_val: 0.0658
Run:01 Epoch: 0333 loss_train: 0.0944 loss_val: 0.0664
Run:01 Epoch: 0334 loss_train: 0.1066 loss_val: 0.0660
Run:01 Epoch: 0335 loss_train: 0.1023 loss_val: 0.0637
Run:01 Epoch: 0336 loss_train: 0.0935 loss_val: 0.0645
Run:01 Epoch: 0337 loss_train: 0.0997 loss_val: 0.0630
Run:01 Epoch: 0338 loss_train: 0.1010 loss_val: 0.0635
Run:01 Epoch: 0339 loss_train: 0.0944 loss_val: 0.0650
Run:01 Epoch: 0340 loss_train: 0.1040 loss_val: 0.0659
Run:01 Epoch: 0341 loss_train: 0.0982 loss_val: 0.0633
Run:01 Epoch: 0342 loss_train: 0.0941 loss_val: 0.0665
Run:01 Epoch: 0343 loss_train: 0.0860 loss_val: 0.0676
Run:01 Epoch: 0344 loss_train: 0.1008 loss_val: 0.0645
Run:01 Epoch: 0345 loss_train: 0.0897 loss_val: 0.0647
Run:01 Epoch: 0346 loss_train: 0.0963 loss_val: 0.0661
Run:01 Epoch: 0347 loss_train: 0.0980 loss_val: 0.0631
Run:01 Epoch: 0348 loss_train: 0.0879 loss_val: 0.0669
Run:01 Epoch: 0349 loss_train: 0.0919 loss_val: 0.0676
Run:01 Epoch: 0350 loss_train: 0.1034 loss_val: 0.0694
Run:01 Epoch: 0351 loss_train: 0.0926 loss_val: 0.0676
Run:01 Epoch: 0352 loss_train: 0.0958 loss_val: 0.0680
Run:01 Epoch: 0353 loss_train: 0.0917 loss_val: 0.0634
Run:01 Epoch: 0354 loss_train: 0.0980 loss_val: 0.0657
Run:01 Epoch: 0355 loss_train: 0.0942 loss_val: 0.0642
Run:01 Epoch: 0356 loss_train: 0.1034 loss_val: 0.0618
Run:01 Epoch: 0357 loss_train: 0.0948 loss_val: 0.0634
Run:01 Epoch: 0358 loss_train: 0.0975 loss_val: 0.0619
Run:01 Epoch: 0359 loss_train: 0.0838 loss_val: 0.0651
Run:01 Epoch: 0360 loss_train: 0.1002 loss_val: 0.0665
Run:01 Epoch: 0361 loss_train: 0.0914 loss_val: 0.0654
Run:01 Epoch: 0362 loss_train: 0.0914 loss_val: 0.0665
Run:01 Epoch: 0363 loss_train: 0.0998 loss_val: 0.0682
Run:01 Epoch: 0364 loss_train: 0.1006 loss_val: 0.0660
Run:01 Epoch: 0365 loss_train: 0.0903 loss_val: 0.0676
Run:01 Epoch: 0366 loss_train: 0.1032 loss_val: 0.0675
Run:01 Epoch: 0367 loss_train: 0.0804 loss_val: 0.0624
Run:01 Epoch: 0368 loss_train: 0.0993 loss_val: 0.0623
Run:01 Epoch: 0369 loss_train: 0.0958 loss_val: 0.0617
Run:01 Epoch: 0370 loss_train: 0.0862 loss_val: 0.0603
Run:01 Epoch: 0371 loss_train: 0.1071 loss_val: 0.0616
Run:01 Epoch: 0372 loss_train: 0.0854 loss_val: 0.0611
Run:01 Epoch: 0373 loss_train: 0.0863 loss_val: 0.0623
Run:01 Epoch: 0374 loss_train: 0.0934 loss_val: 0.0618
Run:01 Epoch: 0375 loss_train: 0.0871 loss_val: 0.0617
Run:01 Epoch: 0376 loss_train: 0.0820 loss_val: 0.0614
Run:01 Epoch: 0377 loss_train: 0.0963 loss_val: 0.0642
Run:01 Epoch: 0378 loss_train: 0.0914 loss_val: 0.0624
Run:01 Epoch: 0379 loss_train: 0.0934 loss_val: 0.0629
Run:01 Epoch: 0380 loss_train: 0.0917 loss_val: 0.0620
Run:01 Epoch: 0381 loss_train: 0.0879 loss_val: 0.0667
Run:01 Epoch: 0382 loss_train: 0.0959 loss_val: 0.0658
Run:01 Epoch: 0383 loss_train: 0.0916 loss_val: 0.0626
Run:01 Epoch: 0384 loss_train: 0.0886 loss_val: 0.0628
Run:01 Epoch: 0385 loss_train: 0.0876 loss_val: 0.0638
Run:01 Epoch: 0386 loss_train: 0.0848 loss_val: 0.0639
Run:01 Epoch: 0387 loss_train: 0.0922 loss_val: 0.0633
Run:01 Epoch: 0388 loss_train: 0.0917 loss_val: 0.0619
Run:01 Epoch: 0389 loss_train: 0.0945 loss_val: 0.0624
Run:01 Epoch: 0390 loss_train: 0.0953 loss_val: 0.0680
Run:01 Epoch: 0391 loss_train: 0.0928 loss_val: 0.0617
Run:01 Epoch: 0392 loss_train: 0.0890 loss_val: 0.0661
Run:01 Epoch: 0393 loss_train: 0.0915 loss_val: 0.0676
Run:01 Epoch: 0394 loss_train: 0.0950 loss_val: 0.0655
Run:01 Epoch: 0395 loss_train: 0.0874 loss_val: 0.0616
Run:01 Epoch: 0396 loss_train: 0.0960 loss_val: 0.0625
Run:01 Epoch: 0397 loss_train: 0.0925 loss_val: 0.0596
Run:01 Epoch: 0398 loss_train: 0.0976 loss_val: 0.0620
Run:01 Epoch: 0399 loss_train: 0.0873 loss_val: 0.0615
Run:01 Epoch: 0400 loss_train: 0.0884 loss_val: 0.0599
Run:01 Epoch: 0401 loss_train: 0.0917 loss_val: 0.0613
Run:01 Epoch: 0402 loss_train: 0.0914 loss_val: 0.0604
Run:01 Epoch: 0403 loss_train: 0.0913 loss_val: 0.0601
Run:01 Epoch: 0404 loss_train: 0.0945 loss_val: 0.0585
Run:01 Epoch: 0405 loss_train: 0.0932 loss_val: 0.0625
Run:01 Epoch: 0406 loss_train: 0.0891 loss_val: 0.0642
Run:01 Epoch: 0407 loss_train: 0.0937 loss_val: 0.0632
Run:01 Epoch: 0408 loss_train: 0.0909 loss_val: 0.0627
Run:01 Epoch: 0409 loss_train: 0.0950 loss_val: 0.0615
Run:01 Epoch: 0410 loss_train: 0.0874 loss_val: 0.0614
Run:01 Epoch: 0411 loss_train: 0.0867 loss_val: 0.0589
Run:01 Epoch: 0412 loss_train: 0.0871 loss_val: 0.0602
Run:01 Epoch: 0413 loss_train: 0.0913 loss_val: 0.0601
Run:01 Epoch: 0414 loss_train: 0.0898 loss_val: 0.0599
Run:01 Epoch: 0415 loss_train: 0.0885 loss_val: 0.0575
Run:01 Epoch: 0416 loss_train: 0.0854 loss_val: 0.0587
Run:01 Epoch: 0417 loss_train: 0.0913 loss_val: 0.0613
Run:01 Epoch: 0418 loss_train: 0.0833 loss_val: 0.0606
Run:01 Epoch: 0419 loss_train: 0.0922 loss_val: 0.0624
Run:01 Epoch: 0420 loss_train: 0.0900 loss_val: 0.0600
Run:01 Epoch: 0421 loss_train: 0.0832 loss_val: 0.0620
Run:01 Epoch: 0422 loss_train: 0.0894 loss_val: 0.0614
Run:01 Epoch: 0423 loss_train: 0.0885 loss_val: 0.0609
Run:01 Epoch: 0424 loss_train: 0.0870 loss_val: 0.0590
Run:01 Epoch: 0425 loss_train: 0.0951 loss_val: 0.0593
Run:01 Epoch: 0426 loss_train: 0.0883 loss_val: 0.0609
Run:01 Epoch: 0427 loss_train: 0.0968 loss_val: 0.0601
Run:01 Epoch: 0428 loss_train: 0.0759 loss_val: 0.0584
Run:01 Epoch: 0429 loss_train: 0.0933 loss_val: 0.0585
Run:01 Epoch: 0430 loss_train: 0.0947 loss_val: 0.0565
Run:01 Epoch: 0431 loss_train: 0.0941 loss_val: 0.0614
Run:01 Epoch: 0432 loss_train: 0.0894 loss_val: 0.0587
Run:01 Epoch: 0433 loss_train: 0.0922 loss_val: 0.0578
Run:01 Epoch: 0434 loss_train: 0.0904 loss_val: 0.0587
Run:01 Epoch: 0435 loss_train: 0.0863 loss_val: 0.0617
Run:01 Epoch: 0436 loss_train: 0.0854 loss_val: 0.0606
Run:01 Epoch: 0437 loss_train: 0.0890 loss_val: 0.0587
Run:01 Epoch: 0438 loss_train: 0.0814 loss_val: 0.0600
Run:01 Epoch: 0439 loss_train: 0.0932 loss_val: 0.0596
Run:01 Epoch: 0440 loss_train: 0.0830 loss_val: 0.0596
Run Train:  33%|███▎      | 1/3 [1:53:54<3:47:48, 6834.02s/it]/users/Min/HG/src_new2/lahypergcn_highschool_hidden20.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  cvae_features = torch.tensor(X, dtype=torch.float32).to(device)
/users/Min/miniconda/envs/hy/lib/python3.9/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
Run:01 Epoch: 0441 loss_train: 0.0855 loss_val: 0.0606
Run:01 Epoch: 0442 loss_train: 0.0809 loss_val: 0.0590
Run:01 Epoch: 0443 loss_train: 0.0831 loss_val: 0.0582
Run:01 Epoch: 0444 loss_train: 0.0965 loss_val: 0.0575
Run:01 Epoch: 0445 loss_train: 0.0934 loss_val: 0.0563
Run:01 Epoch: 0446 loss_train: 0.0764 loss_val: 0.0587
Run:01 Epoch: 0447 loss_train: 0.0788 loss_val: 0.0600
Run:01 Epoch: 0448 loss_train: 0.0774 loss_val: 0.0592
Run:01 Epoch: 0449 loss_train: 0.0789 loss_val: 0.0581
Run:01 Epoch: 0450 loss_train: 0.1031 loss_val: 0.0604
Run:01 Epoch: 0451 loss_train: 0.0892 loss_val: 0.0583
Run:01 Epoch: 0452 loss_train: 0.0904 loss_val: 0.0609
Run:01 Epoch: 0453 loss_train: 0.0853 loss_val: 0.0602
Run:01 Epoch: 0454 loss_train: 0.0885 loss_val: 0.0585
Run:01 Epoch: 0455 loss_train: 0.0778 loss_val: 0.0601
Run:01 Epoch: 0456 loss_train: 0.0824 loss_val: 0.0613
Run:01 Epoch: 0457 loss_train: 0.0882 loss_val: 0.0596
Run:01 Epoch: 0458 loss_train: 0.0832 loss_val: 0.0582
Run:01 Epoch: 0459 loss_train: 0.0774 loss_val: 0.0561
Run:01 Epoch: 0460 loss_train: 0.1014 loss_val: 0.0547
Run:01 Epoch: 0461 loss_train: 0.0875 loss_val: 0.0571
Run:01 Epoch: 0462 loss_train: 0.1073 loss_val: 0.0577
Run:01 Epoch: 0463 loss_train: 0.0863 loss_val: 0.0566
Run:01 Epoch: 0464 loss_train: 0.0892 loss_val: 0.0550
Run:01 Epoch: 0465 loss_train: 0.0845 loss_val: 0.0573
Run:01 Epoch: 0466 loss_train: 0.0945 loss_val: 0.0612
Run:01 Epoch: 0467 loss_train: 0.0776 loss_val: 0.0594
Run:01 Epoch: 0468 loss_train: 0.0956 loss_val: 0.0590
Run:01 Epoch: 0469 loss_train: 0.0885 loss_val: 0.0580
Run:01 Epoch: 0470 loss_train: 0.0735 loss_val: 0.0579
Run:01 Epoch: 0471 loss_train: 0.0838 loss_val: 0.0571
Run:01 Epoch: 0472 loss_train: 0.0847 loss_val: 0.0591
Run:01 Epoch: 0473 loss_train: 0.0856 loss_val: 0.0533
Run:01 Epoch: 0474 loss_train: 0.0829 loss_val: 0.0579
Run:01 Epoch: 0475 loss_train: 0.0857 loss_val: 0.0577
Run:01 Epoch: 0476 loss_train: 0.0819 loss_val: 0.0611
Run:01 Epoch: 0477 loss_train: 0.0840 loss_val: 0.0558
Run:01 Epoch: 0478 loss_train: 0.0833 loss_val: 0.0583
Run:01 Epoch: 0479 loss_train: 0.0875 loss_val: 0.0602
Run:01 Epoch: 0480 loss_train: 0.0850 loss_val: 0.0590
Run:01 Epoch: 0481 loss_train: 0.0944 loss_val: 0.0640
Run:01 Epoch: 0482 loss_train: 0.0958 loss_val: 0.0599
Run:01 Epoch: 0483 loss_train: 0.0888 loss_val: 0.0597
Run:01 Epoch: 0484 loss_train: 0.0870 loss_val: 0.0563
Run:01 Epoch: 0485 loss_train: 0.0827 loss_val: 0.0571
Run:01 Epoch: 0486 loss_train: 0.0833 loss_val: 0.0557
Run:01 Epoch: 0487 loss_train: 0.0918 loss_val: 0.0564
Run:01 Epoch: 0488 loss_train: 0.0889 loss_val: 0.0567
Run:01 Epoch: 0489 loss_train: 0.0819 loss_val: 0.0580
Run:01 Epoch: 0490 loss_train: 0.0862 loss_val: 0.0579
Run:01 Epoch: 0491 loss_train: 0.0860 loss_val: 0.0572
Run:01 Epoch: 0492 loss_train: 0.0900 loss_val: 0.0588
Run:01 Epoch: 0493 loss_train: 0.0833 loss_val: 0.0581
Run:01 Epoch: 0494 loss_train: 0.0806 loss_val: 0.0552
Run:01 Epoch: 0495 loss_train: 0.0891 loss_val: 0.0566
Run:01 Epoch: 0496 loss_train: 0.0808 loss_val: 0.0568
Run:01 Epoch: 0497 loss_train: 0.0900 loss_val: 0.0555
Run:01 Epoch: 0498 loss_train: 0.0811 loss_val: 0.0570
Run:01 Epoch: 0499 loss_train: 0.0821 loss_val: 0.0577
Run:01 Epoch: 0500 loss_train: 0.0806 loss_val: 0.0572
====================================================================================================
output: torch.Size([327, 9]) lbls: torch.Size([327])
====================================================================================================
model: LAHyperGCN(
  (hygcn1): HyperGCNConv(
    (act): ReLU(inplace=True)
    (drop): Dropout(p=0.5, inplace=False)
    (theta): Linear(in_features=327, out_features=20, bias=True)
  )
  (hygcn2): HyperGCNConv(
    (act): ReLU(inplace=True)
    (drop): Dropout(p=0.5, inplace=False)
    (theta): Linear(in_features=100, out_features=9, bias=True)
  )
)
Run:02 Epoch: 0001 loss_train: 2.2051 loss_val: 2.1921
Run:02 Epoch: 0002 loss_train: 2.1981 loss_val: 2.1902
Run:02 Epoch: 0003 loss_train: 2.1927 loss_val: 2.1881
Run:02 Epoch: 0004 loss_train: 2.1856 loss_val: 2.1852
Run:02 Epoch: 0005 loss_train: 2.1785 loss_val: 2.1828
Run:02 Epoch: 0006 loss_train: 2.1703 loss_val: 2.1784
Run:02 Epoch: 0007 loss_train: 2.1602 loss_val: 2.1737
Run:02 Epoch: 0008 loss_train: 2.1508 loss_val: 2.1672
Run:02 Epoch: 0009 loss_train: 2.1415 loss_val: 2.1600
Run:02 Epoch: 0010 loss_train: 2.1262 loss_val: 2.1494
Run:02 Epoch: 0011 loss_train: 2.1145 loss_val: 2.1383
Run:02 Epoch: 0012 loss_train: 2.1042 loss_val: 2.1226
Run:02 Epoch: 0013 loss_train: 2.0877 loss_val: 2.1055
Run:02 Epoch: 0014 loss_train: 2.0748 loss_val: 2.0861
Run:02 Epoch: 0015 loss_train: 2.0608 loss_val: 2.0642
Run:02 Epoch: 0016 loss_train: 2.0274 loss_val: 2.0387
Run:02 Epoch: 0017 loss_train: 2.0066 loss_val: 2.0101
Run:02 Epoch: 0018 loss_train: 1.9813 loss_val: 1.9785
Run:02 Epoch: 0019 loss_train: 1.9619 loss_val: 1.9447
Run:02 Epoch: 0020 loss_train: 1.9208 loss_val: 1.9095
Run:02 Epoch: 0021 loss_train: 1.8894 loss_val: 1.8696
Run:02 Epoch: 0022 loss_train: 1.8517 loss_val: 1.8292
Run:02 Epoch: 0023 loss_train: 1.8096 loss_val: 1.7851
Run:02 Epoch: 0024 loss_train: 1.7628 loss_val: 1.7348
Run:02 Epoch: 0025 loss_train: 1.7158 loss_val: 1.6818
Run:02 Epoch: 0026 loss_train: 1.6846 loss_val: 1.6241
Run:02 Epoch: 0027 loss_train: 1.6353 loss_val: 1.5761
Run:02 Epoch: 0028 loss_train: 1.5812 loss_val: 1.5177
Run:02 Epoch: 0029 loss_train: 1.5315 loss_val: 1.4619
Run:02 Epoch: 0030 loss_train: 1.4729 loss_val: 1.4106
Run:02 Epoch: 0031 loss_train: 1.4339 loss_val: 1.3579
Run:02 Epoch: 0032 loss_train: 1.3735 loss_val: 1.3117
Run:02 Epoch: 0033 loss_train: 1.3399 loss_val: 1.2621
Run:02 Epoch: 0034 loss_train: 1.2976 loss_val: 1.2168
Run:02 Epoch: 0035 loss_train: 1.2524 loss_val: 1.1714
Run:02 Epoch: 0036 loss_train: 1.1972 loss_val: 1.1243
Run:02 Epoch: 0037 loss_train: 1.1580 loss_val: 1.0714
Run:02 Epoch: 0038 loss_train: 1.0993 loss_val: 1.0236
Run:02 Epoch: 0039 loss_train: 1.0554 loss_val: 0.9776
Run:02 Epoch: 0040 loss_train: 1.0213 loss_val: 0.9368
Run:02 Epoch: 0041 loss_train: 0.9697 loss_val: 0.8886
Run:02 Epoch: 0042 loss_train: 0.9312 loss_val: 0.8478
Run:02 Epoch: 0043 loss_train: 0.8992 loss_val: 0.8069
Run:02 Epoch: 0044 loss_train: 0.8517 loss_val: 0.7752
Run:02 Epoch: 0045 loss_train: 0.8061 loss_val: 0.7485
Run:02 Epoch: 0046 loss_train: 0.8002 loss_val: 0.7106
Run:02 Epoch: 0047 loss_train: 0.7474 loss_val: 0.6706
Run:02 Epoch: 0048 loss_train: 0.7392 loss_val: 0.6355
Run:02 Epoch: 0049 loss_train: 0.6766 loss_val: 0.6114
Run:02 Epoch: 0050 loss_train: 0.6612 loss_val: 0.5814
Run:02 Epoch: 0051 loss_train: 0.6303 loss_val: 0.5596
Run:02 Epoch: 0052 loss_train: 0.5969 loss_val: 0.5371
Run:02 Epoch: 0053 loss_train: 0.5658 loss_val: 0.5143
Run:02 Epoch: 0054 loss_train: 0.5788 loss_val: 0.4909
Run:02 Epoch: 0055 loss_train: 0.5257 loss_val: 0.4674
Run:02 Epoch: 0056 loss_train: 0.4941 loss_val: 0.4472
Run:02 Epoch: 0057 loss_train: 0.4794 loss_val: 0.4221
Run:02 Epoch: 0058 loss_train: 0.4778 loss_val: 0.4098
Run:02 Epoch: 0059 loss_train: 0.4365 loss_val: 0.4000
Run:02 Epoch: 0060 loss_train: 0.4184 loss_val: 0.3809
Run:02 Epoch: 0061 loss_train: 0.3962 loss_val: 0.3651
Run:02 Epoch: 0062 loss_train: 0.4052 loss_val: 0.3413
Run:02 Epoch: 0063 loss_train: 0.3957 loss_val: 0.3285
Run:02 Epoch: 0064 loss_train: 0.3668 loss_val: 0.3183
Run:02 Epoch: 0065 loss_train: 0.3735 loss_val: 0.3159
Run:02 Epoch: 0066 loss_train: 0.3359 loss_val: 0.3074
Run:02 Epoch: 0067 loss_train: 0.3429 loss_val: 0.2953
Run:02 Epoch: 0068 loss_train: 0.3332 loss_val: 0.2839
Run:02 Epoch: 0069 loss_train: 0.3264 loss_val: 0.2675
Run:02 Epoch: 0070 loss_train: 0.3271 loss_val: 0.2595
Run:02 Epoch: 0071 loss_train: 0.3114 loss_val: 0.2491
Run:02 Epoch: 0072 loss_train: 0.2929 loss_val: 0.2482
Run:02 Epoch: 0073 loss_train: 0.2838 loss_val: 0.2346
Run:02 Epoch: 0074 loss_train: 0.2991 loss_val: 0.2309
Run:02 Epoch: 0075 loss_train: 0.2962 loss_val: 0.2251
Run:02 Epoch: 0076 loss_train: 0.2580 loss_val: 0.2197
Run:02 Epoch: 0077 loss_train: 0.2729 loss_val: 0.2130
Run:02 Epoch: 0078 loss_train: 0.2736 