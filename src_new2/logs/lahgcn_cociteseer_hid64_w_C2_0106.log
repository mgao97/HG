args
 Namespace(samples=4, concat=10, runs=1, latent_size=10, dataset='cocitationciteseer', seed=42, epochs=1000, lr=0.01, weight_decay=0.0005, hidden=64, dropout=0.5, batch_size=128, tem=0.5, lam=1.0, pretrain_epochs=8, pretrain_lr=0.05, conditional=True, update_epochs=20, num_models=100, warmup=200, cuda=False)
This is cocitation_citeseer dataset:
  ->  num_classes
  ->  num_vertices
  ->  num_edges
  ->  dim_features
  ->  features
  ->  edge_list
  ->  labels
  ->  train_mask
  ->  val_mask
  ->  test_mask
Please try `data['name']` to get the specified data.
Run Train:   0%|          | 0/1 [00:00<?, ?it/s]Run Train: 100%|██████████| 1/1 [10:20<00:00, 620.93s/it]Run Train: 100%|██████████| 1/1 [10:20<00:00, 620.93s/it]
Run:01 Epoch: 0001 loss_train: 1.7899 loss_val: 1.7831
Run:01 Epoch: 0011 loss_train: 1.5845 loss_val: 1.6032
Run:01 Epoch: 0021 loss_train: 1.2749 loss_val: 1.3755
Run:01 Epoch: 0031 loss_train: 1.2063 loss_val: 1.3536
Run:01 Epoch: 0041 loss_train: 1.1752 loss_val: 1.3578
Run:01 Epoch: 0051 loss_train: 1.1660 loss_val: 1.3571
Run:01 Epoch: 0061 loss_train: 1.1617 loss_val: 1.3578
Run:01 Epoch: 0071 loss_train: 1.1603 loss_val: 1.3571
Run:01 Epoch: 0081 loss_train: 1.1590 loss_val: 1.3547
Run:01 Epoch: 0091 loss_train: 1.1605 loss_val: 1.3602
Run:01 Epoch: 0101 loss_train: 1.1605 loss_val: 1.3563
Run:01 Epoch: 0111 loss_train: 1.1588 loss_val: 1.3582
Run:01 Epoch: 0121 loss_train: 1.1595 loss_val: 1.3613
Run:01 Epoch: 0131 loss_train: 1.1584 loss_val: 1.3606
Run:01 Epoch: 0141 loss_train: 1.1595 loss_val: 1.3598
Run:01 Epoch: 0151 loss_train: 1.1599 loss_val: 1.3589
Run:01 Epoch: 0161 loss_train: 1.1588 loss_val: 1.3585
Run:01 Epoch: 0171 loss_train: 1.1592 loss_val: 1.3610
Run:01 Epoch: 0181 loss_train: 1.1591 loss_val: 1.3584
Run:01 Epoch: 0191 loss_train: 1.1592 loss_val: 1.3591
Run:01 Epoch: 0201 loss_train: 1.1582 loss_val: 1.3591
Run:01 Epoch: 0211 loss_train: 1.1590 loss_val: 1.3582
Run:01 Epoch: 0221 loss_train: 1.1600 loss_val: 1.3598
Run:01 Epoch: 0231 loss_train: 1.1595 loss_val: 1.3588
Run:01 Epoch: 0241 loss_train: 1.1595 loss_val: 1.3576
Run:01 Epoch: 0251 loss_train: 1.1592 loss_val: 1.3592
Run:01 Epoch: 0261 loss_train: 1.1593 loss_val: 1.3614
Run:01 Epoch: 0271 loss_train: 1.1590 loss_val: 1.3612
Run:01 Epoch: 0281 loss_train: 1.1594 loss_val: 1.3589
Run:01 Epoch: 0291 loss_train: 1.1591 loss_val: 1.3621
Run:01 Epoch: 0301 loss_train: 1.1594 loss_val: 1.3589
Run:01 Epoch: 0311 loss_train: 1.1588 loss_val: 1.3615
Run:01 Epoch: 0321 loss_train: 1.1584 loss_val: 1.3592
Run:01 Epoch: 0331 loss_train: 1.1611 loss_val: 1.3576
Run:01 Epoch: 0341 loss_train: 1.1585 loss_val: 1.3583
Run:01 Epoch: 0351 loss_train: 1.1588 loss_val: 1.3571
Run:01 Epoch: 0361 loss_train: 1.1589 loss_val: 1.3600
Run:01 Epoch: 0371 loss_train: 1.1586 loss_val: 1.3604
Run:01 Epoch: 0381 loss_train: 1.1599 loss_val: 1.3589
Run:01 Epoch: 0391 loss_train: 1.1592 loss_val: 1.3571
Run:01 Epoch: 0401 loss_train: 1.1587 loss_val: 1.3602
Run:01 Epoch: 0411 loss_train: 1.1589 loss_val: 1.3579
Run:01 Epoch: 0421 loss_train: 1.1586 loss_val: 1.3603
Run:01 Epoch: 0431 loss_train: 1.1590 loss_val: 1.3608
Run:01 Epoch: 0441 loss_train: 1.1582 loss_val: 1.3610
Run:01 Epoch: 0451 loss_train: 1.1596 loss_val: 1.3578
Run:01 Epoch: 0461 loss_train: 1.1591 loss_val: 1.3590
Run:01 Epoch: 0471 loss_train: 1.1591 loss_val: 1.3584
Run:01 Epoch: 0481 loss_train: 1.1599 loss_val: 1.3619
Run:01 Epoch: 0491 loss_train: 1.1588 loss_val: 1.3582
Run:01 Epoch: 0501 loss_train: 1.1584 loss_val: 1.3608
Run:01 Epoch: 0511 loss_train: 1.1591 loss_val: 1.3599
Run:01 Epoch: 0521 loss_train: 1.1589 loss_val: 1.3616
Run:01 Epoch: 0531 loss_train: 1.1590 loss_val: 1.3611
Run:01 Epoch: 0541 loss_train: 1.1586 loss_val: 1.3602
Run:01 Epoch: 0551 loss_train: 1.1586 loss_val: 1.3595
Run:01 Epoch: 0561 loss_train: 1.1602 loss_val: 1.3593
Run:01 Epoch: 0571 loss_train: 1.1582 loss_val: 1.3585
Run:01 Epoch: 0581 loss_train: 1.1592 loss_val: 1.3600
Run:01 Epoch: 0591 loss_train: 1.1602 loss_val: 1.3579
Run:01 Epoch: 0601 loss_train: 1.1599 loss_val: 1.3581
Run:01 Epoch: 0611 loss_train: 1.1591 loss_val: 1.3588
Run:01 Epoch: 0621 loss_train: 1.1601 loss_val: 1.3605
Run:01 Epoch: 0631 loss_train: 1.1582 loss_val: 1.3606
Run:01 Epoch: 0641 loss_train: 1.1585 loss_val: 1.3605
Run:01 Epoch: 0651 loss_train: 1.1606 loss_val: 1.3565
Run:01 Epoch: 0661 loss_train: 1.1596 loss_val: 1.3624
Run:01 Epoch: 0671 loss_train: 1.1599 loss_val: 1.3568
Run:01 Epoch: 0681 loss_train: 1.1583 loss_val: 1.3576
Run:01 Epoch: 0691 loss_train: 1.1589 loss_val: 1.3587
Run:01 Epoch: 0701 loss_train: 1.1586 loss_val: 1.3594
Run:01 Epoch: 0711 loss_train: 1.1586 loss_val: 1.3582
Run:01 Epoch: 0721 loss_train: 1.1579 loss_val: 1.3610
Run:01 Epoch: 0731 loss_train: 1.1593 loss_val: 1.3569
Run:01 Epoch: 0741 loss_train: 1.1587 loss_val: 1.3580
Run:01 Epoch: 0751 loss_train: 1.1597 loss_val: 1.3543
Run:01 Epoch: 0761 loss_train: 1.1591 loss_val: 1.3571
Run:01 Epoch: 0771 loss_train: 1.1590 loss_val: 1.3589
Run:01 Epoch: 0781 loss_train: 1.1593 loss_val: 1.3572
Run:01 Epoch: 0791 loss_train: 1.1582 loss_val: 1.3592
Run:01 Epoch: 0801 loss_train: 1.1587 loss_val: 1.3578
Run:01 Epoch: 0811 loss_train: 1.1585 loss_val: 1.3618
Run:01 Epoch: 0821 loss_train: 1.1594 loss_val: 1.3587
Run:01 Epoch: 0831 loss_train: 1.1584 loss_val: 1.3606
Run:01 Epoch: 0841 loss_train: 1.1589 loss_val: 1.3588
Run:01 Epoch: 0851 loss_train: 1.1593 loss_val: 1.3591
Run:01 Epoch: 0861 loss_train: 1.1588 loss_val: 1.3588
Run:01 Epoch: 0871 loss_train: 1.1584 loss_val: 1.3611
Run:01 Epoch: 0881 loss_train: 1.1601 loss_val: 1.3575
Run:01 Epoch: 0891 loss_train: 1.1598 loss_val: 1.3581
Run:01 Epoch: 0901 loss_train: 1.1578 loss_val: 1.3625
Run:01 Epoch: 0911 loss_train: 1.1582 loss_val: 1.3585
Run:01 Epoch: 0921 loss_train: 1.1583 loss_val: 1.3627
Run:01 Epoch: 0931 loss_train: 1.1590 loss_val: 1.3595
Run:01 Epoch: 0941 loss_train: 1.1585 loss_val: 1.3607
Run:01 Epoch: 0951 loss_train: 1.1590 loss_val: 1.3605
Run:01 Epoch: 0961 loss_train: 1.1591 loss_val: 1.3580
Run:01 Epoch: 0971 loss_train: 1.1584 loss_val: 1.3589
Run:01 Epoch: 0981 loss_train: 1.1593 loss_val: 1.3574
Run:01 Epoch: 0991 loss_train: 1.1592 loss_val: 1.3580
Ablation study with C2 on CocitationCiteseer dataset:


test acc: 0.43478260869565216 test acc std 0.0


test micro f1: 0.43478260869565216 test macro f1 0.3999072675228195
