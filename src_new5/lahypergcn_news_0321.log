/home/ad/miniconda/envs/hy/lib/python3.9/site-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.
  warnings.warn(msg)
args
 Namespace(samples=10, concat=4, runs=1, latent_size=10, dataset='news20', seed=42, epochs=400, lr=0.001, weight_decay=0.0005, hidden=64, dropout=0.5, batch_size=128, tem=0.5, lam=1.0, pretrain_epochs=10, pretrain_lr=0.01, conditional=True, update_epochs=20, num_models=100, warmup=200, dname='20newsW100', add_self_loop=True, exclude_self=False, normtype='all_one', use_mediator=False, cuda=True)
data: Data(x=[16242, 100], edge_index=[2, 65451], y=[16242], n_x=16242, train_percent=0.01, num_hyperedges=100, totedges=100, norm=[65451])
(16242, 100)
[[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 1.]
 [0. 0. 0. ... 0. 0. 0.]]
Hypergraph(num_v=16242, num_e=100)
Hypergraph(num_v=16242, num_e=100)
Run Train:   0%|          | 0/1 [00:00<?, ?it/s]Run:01 Epoch: 0001 loss_train: 1.3904 loss_val: 1.3862
Run:01 Epoch: 0002 loss_train: 1.3876 loss_val: 1.3833
Run:01 Epoch: 0003 loss_train: 1.3846 loss_val: 1.3805
Run:01 Epoch: 0004 loss_train: 1.3819 loss_val: 1.3777
Run:01 Epoch: 0005 loss_train: 1.3793 loss_val: 1.3750
Run:01 Epoch: 0006 loss_train: 1.3769 loss_val: 1.3723
Run:01 Epoch: 0007 loss_train: 1.3745 loss_val: 1.3697
Run:01 Epoch: 0008 loss_train: 1.3723 loss_val: 1.3672
Run:01 Epoch: 0009 loss_train: 1.3704 loss_val: 1.3647
Run:01 Epoch: 0010 loss_train: 1.3682 loss_val: 1.3624
Run:01 Epoch: 0011 loss_train: 1.3664 loss_val: 1.3601
Run:01 Epoch: 0012 loss_train: 1.3650 loss_val: 1.3579
Run:01 Epoch: 0013 loss_train: 1.3638 loss_val: 1.3559
Run:01 Epoch: 0014 loss_train: 1.3624 loss_val: 1.3540
Run:01 Epoch: 0015 loss_train: 1.3615 loss_val: 1.3522
Run:01 Epoch: 0016 loss_train: 1.3609 loss_val: 1.3505
Run:01 Epoch: 0017 loss_train: 1.3599 loss_val: 1.3490
Run:01 Epoch: 0018 loss_train: 1.3597 loss_val: 1.3477
Run:01 Epoch: 0019 loss_train: 1.3591 loss_val: 1.3465
Run:01 Epoch: 0020 loss_train: 1.3591 loss_val: 1.3454
Run:01 Epoch: 0021 loss_train: 1.3588 loss_val: 1.3444
Run:01 Epoch: 0022 loss_train: 1.3589 loss_val: 1.3436
Run:01 Epoch: 0023 loss_train: 1.3589 loss_val: 1.3428
Run:01 Epoch: 0024 loss_train: 1.3587 loss_val: 1.3420
Run:01 Epoch: 0025 loss_train: 1.3585 loss_val: 1.3413
Run:01 Epoch: 0026 loss_train: 1.3581 loss_val: 1.3405
Run:01 Epoch: 0027 loss_train: 1.3582 loss_val: 1.3398
Run:01 Epoch: 0028 loss_train: 1.3579 loss_val: 1.3390
Run:01 Epoch: 0029 loss_train: 1.3577 loss_val: 1.3383
Run:01 Epoch: 0030 loss_train: 1.3571 loss_val: 1.3375
Run:01 Epoch: 0031 loss_train: 1.3574 loss_val: 1.3367
Run:01 Epoch: 0032 loss_train: 1.3575 loss_val: 1.3360
Run:01 Epoch: 0033 loss_train: 1.3573 loss_val: 1.3353
Run:01 Epoch: 0034 loss_train: 1.3579 loss_val: 1.3347
Run:01 Epoch: 0035 loss_train: 1.3587 loss_val: 1.3342
Run:01 Epoch: 0036 loss_train: 1.3594 loss_val: 1.3337
Run:01 Epoch: 0037 loss_train: 1.3605 loss_val: 1.3334
Run:01 Epoch: 0038 loss_train: 1.3620 loss_val: 1.3332
Run:01 Epoch: 0039 loss_train: 1.3637 loss_val: 1.3332
Run:01 Epoch: 0040 loss_train: 1.3664 loss_val: 1.3334
Run:01 Epoch: 0041 loss_train: 1.3690 loss_val: 1.3337
Run:01 Epoch: 0042 loss_train: 1.3723 loss_val: 1.3342
Run:01 Epoch: 0043 loss_train: 1.3755 loss_val: 1.3349
Run:01 Epoch: 0044 loss_train: 1.3789 loss_val: 1.3357
Run:01 Epoch: 0045 loss_train: 1.3829 loss_val: 1.3367
Run:01 Epoch: 0046 loss_train: 1.3870 loss_val: 1.3377
Run:01 Epoch: 0047 loss_train: 1.3910 loss_val: 1.3389
Run:01 Epoch: 0048 loss_train: 1.3953 loss_val: 1.3400
Run:01 Epoch: 0049 loss_train: 1.3995 loss_val: 1.3411
Run:01 Epoch: 0050 loss_train: 1.4028 loss_val: 1.3420
Run:01 Epoch: 0051 loss_train: 1.4064 loss_val: 1.3426
Run:01 Epoch: 0052 loss_train: 1.4094 loss_val: 1.3428
Run:01 Epoch: 0053 loss_train: 1.4112 loss_val: 1.3423
Run:01 Epoch: 0054 loss_train: 1.4115 loss_val: 1.3410
Run:01 Epoch: 0055 loss_train: 1.4107 loss_val: 1.3389
Run:01 Epoch: 0056 loss_train: 1.4098 loss_val: 1.3359
Run:01 Epoch: 0057 loss_train: 1.4071 loss_val: 1.3321
Run:01 Epoch: 0058 loss_train: 1.4034 loss_val: 1.3275
Run:01 Epoch: 0059 loss_train: 1.3985 loss_val: 1.3221
Run:01 Epoch: 0060 loss_train: 1.3930 loss_val: 1.3162
Run:01 Epoch: 0061 loss_train: 1.3857 loss_val: 1.3098
Run:01 Epoch: 0062 loss_train: 1.3789 loss_val: 1.3031
Run:01 Epoch: 0063 loss_train: 1.3708 loss_val: 1.2961
Run:01 Epoch: 0064 loss_train: 1.3634 loss_val: 1.2890
Run:01 Epoch: 0065 loss_train: 1.3544 loss_val: 1.2818
Run:01 Epoch: 0066 loss_train: 1.3459 loss_val: 1.2747
Run:01 Epoch: 0067 loss_train: 1.3380 loss_val: 1.2675
Run:01 Epoch: 0068 loss_train: 1.3292 loss_val: 1.2605
Run:01 Epoch: 0069 loss_train: 1.3210 loss_val: 1.2535
Run:01 Epoch: 0070 loss_train: 1.3130 loss_val: 1.2467
Run:01 Epoch: 0071 loss_train: 1.3047 loss_val: 1.2399
Run:01 Epoch: 0072 loss_train: 1.2970 loss_val: 1.2332
Run:01 Epoch: 0073 loss_train: 1.2890 loss_val: 1.2265
Run:01 Epoch: 0074 loss_train: 1.2829 loss_val: 1.2199
Run:01 Epoch: 0075 loss_train: 1.2754 loss_val: 1.2133
Run:01 Epoch: 0076 loss_train: 1.2685 loss_val: 1.2067
Run:01 Epoch: 0077 loss_train: 1.2614 loss_val: 1.2001
Run:01 Epoch: 0078 loss_train: 1.2544 loss_val: 1.1934
Run:01 Epoch: 0079 loss_train: 1.2480 loss_val: 1.1867
Run:01 Epoch: 0080 loss_train: 1.2412 loss_val: 1.1799
Run:01 Epoch: 0081 loss_train: 1.2346 loss_val: 1.1730
Run:01 Epoch: 0082 loss_train: 1.2276 loss_val: 1.1660
Run:01 Epoch: 0083 loss_train: 1.2209 loss_val: 1.1589
Run:01 Epoch: 0084 loss_train: 1.2135 loss_val: 1.1517
Run:01 Epoch: 0085 loss_train: 1.2076 loss_val: 1.1444
Run:01 Epoch: 0086 loss_train: 1.2004 loss_val: 1.1371
Run:01 Epoch: 0087 loss_train: 1.1928 loss_val: 1.1297
Run:01 Epoch: 0088 loss_train: 1.1863 loss_val: 1.1223
Run:01 Epoch: 0089 loss_train: 1.1796 loss_val: 1.1149
Run:01 Epoch: 0090 loss_train: 1.1733 loss_val: 1.1074
Run:01 Epoch: 0091 loss_train: 1.1662 loss_val: 1.1000
Run:01 Epoch: 0092 loss_train: 1.1601 loss_val: 1.0925
Run:01 Epoch: 0093 loss_train: 1.1539 loss_val: 1.0852
Run:01 Epoch: 0094 loss_train: 1.1482 loss_val: 1.0778
Run:01 Epoch: 0095 loss_train: 1.1417 loss_val: 1.0706
Run:01 Epoch: 0096 loss_train: 1.1355 loss_val: 1.0634
Run:01 Epoch: 0097 loss_train: 1.1299 loss_val: 1.0563
Run:01 Epoch: 0098 loss_train: 1.1238 loss_val: 1.0492
Run:01 Epoch: 0099 loss_train: 1.1189 loss_val: 1.0423
Run:01 Epoch: 0100 loss_train: 1.1126 loss_val: 1.0354
Run:01 Epoch: 0101 loss_train: 1.1078 loss_val: 1.0286
Run:01 Epoch: 0102 loss_train: 1.1016 loss_val: 1.0220
Run:01 Epoch: 0103 loss_train: 1.0963 loss_val: 1.0154
Run:01 Epoch: 0104 loss_train: 1.0906 loss_val: 1.0090
Run:01 Epoch: 0105 loss_train: 1.0852 loss_val: 1.0026
Run:01 Epoch: 0106 loss_train: 1.0805 loss_val: 0.9962
Run:01 Epoch: 0107 loss_train: 1.0745 loss_val: 0.9898
Run:01 Epoch: 0108 loss_train: 1.0690 loss_val: 0.9834
Run:01 Epoch: 0109 loss_train: 1.0630 loss_val: 0.9770
Run:01 Epoch: 0110 loss_train: 1.0584 loss_val: 0.9706
Run:01 Epoch: 0111 loss_train: 1.0531 loss_val: 0.9642
Run:01 Epoch: 0112 loss_train: 1.0478 loss_val: 0.9578
Run:01 Epoch: 0113 loss_train: 1.0416 loss_val: 0.9515
Run:01 Epoch: 0114 loss_train: 1.0369 loss_val: 0.9452
Run:01 Epoch: 0115 loss_train: 1.0315 loss_val: 0.9389
Run:01 Epoch: 0116 loss_train: 1.0259 loss_val: 0.9328
Run:01 Epoch: 0117 loss_train: 1.0210 loss_val: 0.9268
Run:01 Epoch: 0118 loss_train: 1.0154 loss_val: 0.9208
Run:01 Epoch: 0119 loss_train: 1.0103 loss_val: 0.9150
Run:01 Epoch: 0120 loss_train: 1.0057 loss_val: 0.9093
Run:01 Epoch: 0121 loss_train: 0.9988 loss_val: 0.9036
Run:01 Epoch: 0122 loss_train: 0.9948 loss_val: 0.8980
Run:01 Epoch: 0123 loss_train: 0.9884 loss_val: 0.8924
Run:01 Epoch: 0124 loss_train: 0.9839 loss_val: 0.8868
Run:01 Epoch: 0125 loss_train: 0.9788 loss_val: 0.8813
Run:01 Epoch: 0126 loss_train: 0.9737 loss_val: 0.8759
Run:01 Epoch: 0127 loss_train: 0.9691 loss_val: 0.8705
Run:01 Epoch: 0128 loss_train: 0.9643 loss_val: 0.8652
Run:01 Epoch: 0129 loss_train: 0.9595 loss_val: 0.8599
Run:01 Epoch: 0130 loss_train: 0.9542 loss_val: 0.8548
Run:01 Epoch: 0131 loss_train: 0.9495 loss_val: 0.8498
Run:01 Epoch: 0132 loss_train: 0.9452 loss_val: 0.8449
Run:01 Epoch: 0133 loss_train: 0.9412 loss_val: 0.8401
Run:01 Epoch: 0134 loss_train: 0.9346 loss_val: 0.8354
Run:01 Epoch: 0135 loss_train: 0.9313 loss_val: 0.8307
Run:01 Epoch: 0136 loss_train: 0.9265 loss_val: 0.8260
Run:01 Epoch: 0137 loss_train: 0.9208 loss_val: 0.8214
Run:01 Epoch: 0138 loss_train: 0.9180 loss_val: 0.8168
Run:01 Epoch: 0139 loss_train: 0.9146 loss_val: 0.8124
Run:01 Epoch: 0140 loss_train: 0.9094 loss_val: 0.8081
Run:01 Epoch: 0141 loss_train: 0.9059 loss_val: 0.8039
Run:01 Epoch: 0142 loss_train: 0.9008 loss_val: 0.7998
Run:01 Epoch: 0143 loss_train: 0.8962 loss_val: 0.7957
Run:01 Epoch: 0144 loss_train: 0.8922 loss_val: 0.7916
Run:01 Epoch: 0145 loss_train: 0.8882 loss_val: 0.7875
Run:01 Epoch: 0146 loss_train: 0.8837 loss_val: 0.7835
Run:01 Epoch: 0147 loss_train: 0.8814 loss_val: 0.7797
Run:01 Epoch: 0148 loss_train: 0.8774 loss_val: 0.7760
Run:01 Epoch: 0149 loss_train: 0.8740 loss_val: 0.7723
Run:01 Epoch: 0150 loss_train: 0.8710 loss_val: 0.7687
Run:01 Epoch: 0151 loss_train: 0.8667 loss_val: 0.7651
Run:01 Epoch: 0152 loss_train: 0.8629 loss_val: 0.7615
Run:01 Epoch: 0153 loss_train: 0.8590 loss_val: 0.7580
Run:01 Epoch: 0154 loss_train: 0.8560 loss_val: 0.7547
Run:01 Epoch: 0155 loss_train: 0.8534 loss_val: 0.7514
Run:01 Epoch: 0156 loss_train: 0.8491 loss_val: 0.7482
Run:01 Epoch: 0157 loss_train: 0.8462 loss_val: 0.7450
Run:01 Epoch: 0158 loss_train: 0.8425 loss_val: 0.7419
Run:01 Epoch: 0159 loss_train: 0.8402 loss_val: 0.7389
Run:01 Epoch: 0160 loss_train: 0.8374 loss_val: 0.7359
Run:01 Epoch: 0161 loss_train: 0.8334 loss_val: 0.7329
Run:01 Epoch: 0162 loss_train: 0.8314 loss_val: 0.7300
Run:01 Epoch: 0163 loss_train: 0.8277 loss_val: 0.7271
Run:01 Epoch: 0164 loss_train: 0.8249 loss_val: 0.7244
Run:01 Epoch: 0165 loss_train: 0.8214 loss_val: 0.7217
Run:01 Epoch: 0166 loss_train: 0.8190 loss_val: 0.7191
Run:01 Epoch: 0167 loss_train: 0.8162 loss_val: 0.7165
Run:01 Epoch: 0168 loss_train: 0.8131 loss_val: 0.7138
Run:01 Epoch: 0169 loss_train: 0.8115 loss_val: 0.7113
Run:01 Epoch: 0170 loss_train: 0.8078 loss_val: 0.7088
Run:01 Epoch: 0171 loss_train: 0.8050 loss_val: 0.7064
Run:01 Epoch: 0172 loss_train: 0.8031 loss_val: 0.7040
Run:01 Epoch: 0173 loss_train: 0.8002 loss_val: 0.7018
Run:01 Epoch: 0174 loss_train: 0.7981 loss_val: 0.6996
Run:01 Epoch: 0175 loss_train: 0.7959 loss_val: 0.6974
Run:01 Epoch: 0176 loss_train: 0.7942 loss_val: 0.6951
Run:01 Epoch: 0177 loss_train: 0.7917 loss_val: 0.6929
Run:01 Epoch: 0178 loss_train: 0.7888 loss_val: 0.6907
Run:01 Epoch: 0179 loss_train: 0.7869 loss_val: 0.6887
Run:01 Epoch: 0180 loss_train: 0.7845 loss_val: 0.6867
Run:01 Epoch: 0181 loss_train: 0.7823 loss_val: 0.6847
Run:01 Epoch: 0182 loss_train: 0.7801 loss_val: 0.6827
Run:01 Epoch: 0183 loss_train: 0.7780 loss_val: 0.6807
Run:01 Epoch: 0184 loss_train: 0.7758 loss_val: 0.6788
Run:01 Epoch: 0185 loss_train: 0.7754 loss_val: 0.6769
Run:01 Epoch: 0186 loss_train: 0.7716 loss_val: 0.6752
Run:01 Epoch: 0187 loss_train: 0.7699 loss_val: 0.6735
Run:01 Epoch: 0188 loss_train: 0.7680 loss_val: 0.6719
Run:01 Epoch: 0189 loss_train: 0.7680 loss_val: 0.6702
Run:01 Epoch: 0190 loss_train: 0.7627 loss_val: 0.6686
Run:01 Epoch: 0191 loss_train: 0.7614 loss_val: 0.6669
Run:01 Epoch: 0192 loss_train: 0.7611 loss_val: 0.6652
Run:01 Epoch: 0193 loss_train: 0.7589 loss_val: 0.6635
Run:01 Epoch: 0194 loss_train: 0.7569 loss_val: 0.6618
Run:01 Epoch: 0195 loss_train: 0.7536 loss_val: 0.6602
Run:01 Epoch: 0196 loss_train: 0.7528 loss_val: 0.6587
Run:01 Epoch: 0197 loss_train: 0.7511 loss_val: 0.6573
Run:01 Epoch: 0198 loss_train: 0.7487 loss_val: 0.6559
Run:01 Epoch: 0199 loss_train: 0.7480 loss_val: 0.6546
Run:01 Epoch: 0200 loss_train: 0.7468 loss_val: 0.6532
Run:01 Epoch: 0201 loss_train: 0.7448 loss_val: 0.6519
Run:01 Epoch: 0202 loss_train: 0.7427 loss_val: 0.6505
Run:01 Epoch: 0203 loss_train: 0.7414 loss_val: 0.6492
Run:01 Epoch: 0204 loss_train: 0.7404 loss_val: 0.6479
Run:01 Epoch: 0205 loss_train: 0.7381 loss_val: 0.6466
Run:01 Epoch: 0206 loss_train: 0.7359 loss_val: 0.6454
Run:01 Epoch: 0207 loss_train: 0.7358 loss_val: 0.6442
Run:01 Epoch: 0208 loss_train: 0.7353 loss_val: 0.6430
Run:01 Epoch: 0209 loss_train: 0.7330 loss_val: 0.6418
Run:01 Epoch: 0210 loss_train: 0.7321 loss_val: 0.6406
Run:01 Epoch: 0211 loss_train: 0.7293 loss_val: 0.6395
Run:01 Epoch: 0212 loss_train: 0.7282 loss_val: 0.6384
Run:01 Epoch: 0213 loss_train: 0.7273 loss_val: 0.6373
Run:01 Epoch: 0214 loss_train: 0.7266 loss_val: 0.6361
Run:01 Epoch: 0215 loss_train: 0.7243 loss_val: 0.6351
Run:01 Epoch: 0216 loss_train: 0.7235 loss_val: 0.6341
Run:01 Epoch: 0217 loss_train: 0.7215 loss_val: 0.6331
Run:01 Epoch: 0218 loss_train: 0.7209 loss_val: 0.6321
Run:01 Epoch: 0219 loss_train: 0.7194 loss_val: 0.6311
Run:01 Epoch: 0220 loss_train: 0.7186 loss_val: 0.6301
Run:01 Epoch: 0221 loss_train: 0.7172 loss_val: 0.6292
Run:01 Epoch: 0222 loss_train: 0.7140 loss_val: 0.6282
Run:01 Epoch: 0223 loss_train: 0.7159 loss_val: 0.6272
Run:01 Epoch: 0224 loss_train: 0.7145 loss_val: 0.6262
Run:01 Epoch: 0225 loss_train: 0.7125 loss_val: 0.6253
Run:01 Epoch: 0226 loss_train: 0.7100 loss_val: 0.6244
Run:01 Epoch: 0227 loss_train: 0.7096 loss_val: 0.6235
Run:01 Epoch: 0228 loss_train: 0.7085 loss_val: 0.6227
Run:01 Epoch: 0229 loss_train: 0.7071 loss_val: 0.6219
Run:01 Epoch: 0230 loss_train: 0.7084 loss_val: 0.6211
Run:01 Epoch: 0231 loss_train: 0.7056 loss_val: 0.6204
Run:01 Epoch: 0232 loss_train: 0.7057 loss_val: 0.6197
Run:01 Epoch: 0233 loss_train: 0.7041 loss_val: 0.6189
Run:01 Epoch: 0234 loss_train: 0.7027 loss_val: 0.6180
Run:01 Epoch: 0235 loss_train: 0.7013 loss_val: 0.6172
Run:01 Epoch: 0236 loss_train: 0.7005 loss_val: 0.6164
Run:01 Epoch: 0237 loss_train: 0.6996 loss_val: 0.6156
Run:01 Epoch: 0238 loss_train: 0.6992 loss_val: 0.6148
Run:01 Epoch: 0239 loss_train: 0.6982 loss_val: 0.6141
Run:01 Epoch: 0240 loss_train: 0.6972 loss_val: 0.6134
Run:01 Epoch: 0241 loss_train: 0.6958 loss_val: 0.6128
Run:01 Epoch: 0242 loss_train: 0.6953 loss_val: 0.6122
Run:01 Epoch: 0243 loss_train: 0.6937 loss_val: 0.6116
Run:01 Epoch: 0244 loss_train: 0.6923 loss_val: 0.6109
Run:01 Epoch: 0245 loss_train: 0.6909 loss_val: 0.6103
Run:01 Epoch: 0246 loss_train: 0.6913 loss_val: 0.6096
Run:01 Epoch: 0247 loss_train: 0.6907 loss_val: 0.6089
Run:01 Epoch: 0248 loss_train: 0.6909 loss_val: 0.6083
Run:01 Epoch: 0249 loss_train: 0.6891 loss_val: 0.6077
Run:01 Epoch: 0250 loss_train: 0.6887 loss_val: 0.6071
Run:01 Epoch: 0251 loss_train: 0.6858 loss_val: 0.6065
Run:01 Epoch: 0252 loss_train: 0.6850 loss_val: 0.6059
Run:01 Epoch: 0253 loss_train: 0.6862 loss_val: 0.6053
Run:01 Epoch: 0254 loss_train: 0.6846 loss_val: 0.6047
Run:01 Epoch: 0255 loss_train: 0.6846 loss_val: 0.6042
Run:01 Epoch: 0256 loss_train: 0.6829 loss_val: 0.6037
Run:01 Epoch: 0257 loss_train: 0.6818 loss_val: 0.6032
Run:01 Epoch: 0258 loss_train: 0.6799 loss_val: 0.6027
Run:01 Epoch: 0259 loss_train: 0.6806 loss_val: 0.6022
Run:01 Epoch: 0260 loss_train: 0.6806 loss_val: 0.6017
Run:01 Epoch: 0261 loss_train: 0.6803 loss_val: 0.6012
Run:01 Epoch: 0262 loss_train: 0.6776 loss_val: 0.6006
Run:01 Epoch: 0263 loss_train: 0.6781 loss_val: 0.6001
Run:01 Epoch: 0264 loss_train: 0.6786 loss_val: 0.5996
Run:01 Epoch: 0265 loss_train: 0.6757 loss_val: 0.5992
Run:01 Epoch: 0266 loss_train: 0.6761 loss_val: 0.5988
Run:01 Epoch: 0267 loss_train: 0.6746 loss_val: 0.5983
Run:01 Epoch: 0268 loss_train: 0.6755 loss_val: 0.5979
Run:01 Epoch: 0269 loss_train: 0.6747 loss_val: 0.5974
Run:01 Epoch: 0270 loss_train: 0.6743 loss_val: 0.5970
Run:01 Epoch: 0271 loss_train: 0.6723 loss_val: 0.5965
Run:01 Epoch: 0272 loss_train: 0.6706 loss_val: 0.5960
Run:01 Epoch: 0273 loss_train: 0.6722 loss_val: 0.5955
Run:01 Epoch: 0274 loss_train: 0.6718 loss_val: 0.5952
Run:01 Epoch: 0275 loss_train: 0.6697 loss_val: 0.5949
Run:01 Epoch: 0276 loss_train: 0.6703 loss_val: 0.5945
Run:01 Epoch: 0277 loss_train: 0.6684 loss_val: 0.5941
Run:01 Epoch: 0278 loss_train: 0.6684 loss_val: 0.5938
Run:01 Epoch: 0279 loss_train: 0.6671 loss_val: 0.5934
Run:01 Epoch: 0280 loss_train: 0.6677 loss_val: 0.5930
Run:01 Epoch: 0281 loss_train: 0.6675 loss_val: 0.5925
Run:01 Epoch: 0282 loss_train: 0.6654 loss_val: 0.5921
Run:01 Epoch: 0283 loss_train: 0.6660 loss_val: 0.5917
Run:01 Epoch: 0284 loss_train: 0.6641 loss_val: 0.5914
Run:01 Epoch: 0285 loss_train: 0.6646 loss_val: 0.5911
Run:01 Epoch: 0286 loss_train: 0.6647 loss_val: 0.5908
Run:01 Epoch: 0287 loss_train: 0.6633 loss_val: 0.5905
Run:01 Epoch: 0288 loss_train: 0.6630 loss_val: 0.5903
Run:01 Epoch: 0289 loss_train: 0.6618 loss_val: 0.5899
Run:01 Epoch: 0290 loss_train: 0.6633 loss_val: 0.5895
Run:01 Epoch: 0291 loss_train: 0.6612 loss_val: 0.5891
Run:01 Epoch: 0292 loss_train: 0.6605 loss_val: 0.5888
Run:01 Epoch: 0293 loss_train: 0.6587 loss_val: 0.5884
Run:01 Epoch: 0294 loss_train: 0.6585 loss_val: 0.5881
Run:01 Epoch: 0295 loss_train: 0.6589 loss_val: 0.5877
Run:01 Epoch: 0296 loss_train: 0.6573 loss_val: 0.5875
Run:01 Epoch: 0297 loss_train: 0.6594 loss_val: 0.5873
Run:01 Epoch: 0298 Run Train: 100%|██████████| 1/1 [4:35:53<00:00, 16553.54s/it]Run Train: 100%|██████████| 1/1 [4:35:53<00:00, 16553.54s/it]
loss_train: 0.6575 loss_val: 0.5870
Run:01 Epoch: 0299 loss_train: 0.6583 loss_val: 0.5867
Run:01 Epoch: 0300 loss_train: 0.6585 loss_val: 0.5864
Run:01 Epoch: 0301 loss_train: 0.6581 loss_val: 0.5860
Run:01 Epoch: 0302 loss_train: 0.6573 loss_val: 0.5857
Run:01 Epoch: 0303 loss_train: 0.6548 loss_val: 0.5853
Run:01 Epoch: 0304 loss_train: 0.6559 loss_val: 0.5851
Run:01 Epoch: 0305 loss_train: 0.6549 loss_val: 0.5849
Run:01 Epoch: 0306 loss_train: 0.6531 loss_val: 0.5847
Run:01 Epoch: 0307 loss_train: 0.6547 loss_val: 0.5844
Run:01 Epoch: 0308 loss_train: 0.6534 loss_val: 0.5843
Run:01 Epoch: 0309 loss_train: 0.6529 loss_val: 0.5841
Run:01 Epoch: 0310 loss_train: 0.6523 loss_val: 0.5838
Run:01 Epoch: 0311 loss_train: 0.6524 loss_val: 0.5835
Run:01 Epoch: 0312 loss_train: 0.6522 loss_val: 0.5831
Run:01 Epoch: 0313 loss_train: 0.6514 loss_val: 0.5828
Run:01 Epoch: 0314 loss_train: 0.6524 loss_val: 0.5825
Run:01 Epoch: 0315 loss_train: 0.6507 loss_val: 0.5822
Run:01 Epoch: 0316 loss_train: 0.6508 loss_val: 0.5820
Run:01 Epoch: 0317 loss_train: 0.6509 loss_val: 0.5818
Run:01 Epoch: 0318 loss_train: 0.6513 loss_val: 0.5816
Run:01 Epoch: 0319 loss_train: 0.6496 loss_val: 0.5814
Run:01 Epoch: 0320 loss_train: 0.6498 loss_val: 0.5811
Run:01 Epoch: 0321 loss_train: 0.6495 loss_val: 0.5809
Run:01 Epoch: 0322 loss_train: 0.6485 loss_val: 0.5806
Run:01 Epoch: 0323 loss_train: 0.6476 loss_val: 0.5804
Run:01 Epoch: 0324 loss_train: 0.6480 loss_val: 0.5802
Run:01 Epoch: 0325 loss_train: 0.6486 loss_val: 0.5799
Run:01 Epoch: 0326 loss_train: 0.6452 loss_val: 0.5798
Run:01 Epoch: 0327 loss_train: 0.6465 loss_val: 0.5796
Run:01 Epoch: 0328 loss_train: 0.6465 loss_val: 0.5794
Run:01 Epoch: 0329 loss_train: 0.6473 loss_val: 0.5791
Run:01 Epoch: 0330 loss_train: 0.6462 loss_val: 0.5789
Run:01 Epoch: 0331 loss_train: 0.6442 loss_val: 0.5787
Run:01 Epoch: 0332 loss_train: 0.6444 loss_val: 0.5784
Run:01 Epoch: 0333 loss_train: 0.6447 loss_val: 0.5782
Run:01 Epoch: 0334 loss_train: 0.6451 loss_val: 0.5780
Run:01 Epoch: 0335 loss_train: 0.6435 loss_val: 0.5778
Run:01 Epoch: 0336 loss_train: 0.6447 loss_val: 0.5776
Run:01 Epoch: 0337 loss_train: 0.6429 loss_val: 0.5775
Run:01 Epoch: 0338 loss_train: 0.6436 loss_val: 0.5773
Run:01 Epoch: 0339 loss_train: 0.6433 loss_val: 0.5772
Run:01 Epoch: 0340 loss_train: 0.6422 loss_val: 0.5770
Run:01 Epoch: 0341 loss_train: 0.6423 loss_val: 0.5767
Run:01 Epoch: 0342 loss_train: 0.6424 loss_val: 0.5765
Run:01 Epoch: 0343 loss_train: 0.6410 loss_val: 0.5763
Run:01 Epoch: 0344 loss_train: 0.6407 loss_val: 0.5761
Run:01 Epoch: 0345 loss_train: 0.6422 loss_val: 0.5759
Run:01 Epoch: 0346 loss_train: 0.6414 loss_val: 0.5758
Run:01 Epoch: 0347 loss_train: 0.6401 loss_val: 0.5756
Run:01 Epoch: 0348 loss_train: 0.6410 loss_val: 0.5754
Run:01 Epoch: 0349 loss_train: 0.6391 loss_val: 0.5752
Run:01 Epoch: 0350 loss_train: 0.6399 loss_val: 0.5750
Run:01 Epoch: 0351 loss_train: 0.6385 loss_val: 0.5748
Run:01 Epoch: 0352 loss_train: 0.6378 loss_val: 0.5747
Run:01 Epoch: 0353 loss_train: 0.6387 loss_val: 0.5745
Run:01 Epoch: 0354 loss_train: 0.6370 loss_val: 0.5744
Run:01 Epoch: 0355 loss_train: 0.6366 loss_val: 0.5742
Run:01 Epoch: 0356 loss_train: 0.6375 loss_val: 0.5741
Run:01 Epoch: 0357 loss_train: 0.6375 loss_val: 0.5738
Run:01 Epoch: 0358 loss_train: 0.6382 loss_val: 0.5737
Run:01 Epoch: 0359 loss_train: 0.6374 loss_val: 0.5736
Run:01 Epoch: 0360 loss_train: 0.6364 loss_val: 0.5735
Run:01 Epoch: 0361 loss_train: 0.6366 loss_val: 0.5734
Run:01 Epoch: 0362 loss_train: 0.6361 loss_val: 0.5732
Run:01 Epoch: 0363 loss_train: 0.6356 loss_val: 0.5730
Run:01 Epoch: 0364 loss_train: 0.6366 loss_val: 0.5728
Run:01 Epoch: 0365 loss_train: 0.6360 loss_val: 0.5726
Run:01 Epoch: 0366 loss_train: 0.6356 loss_val: 0.5724
Run:01 Epoch: 0367 loss_train: 0.6364 loss_val: 0.5722
Run:01 Epoch: 0368 loss_train: 0.6359 loss_val: 0.5720
Run:01 Epoch: 0369 loss_train: 0.6342 loss_val: 0.5719
Run:01 Epoch: 0370 loss_train: 0.6337 loss_val: 0.5717
Run:01 Epoch: 0371 loss_train: 0.6341 loss_val: 0.5716
Run:01 Epoch: 0372 loss_train: 0.6333 loss_val: 0.5715
Run:01 Epoch: 0373 loss_train: 0.6340 loss_val: 0.5715
Run:01 Epoch: 0374 loss_train: 0.6334 loss_val: 0.5713
Run:01 Epoch: 0375 loss_train: 0.6327 loss_val: 0.5713
Run:01 Epoch: 0376 loss_train: 0.6347 loss_val: 0.5712
Run:01 Epoch: 0377 loss_train: 0.6324 loss_val: 0.5710
Run:01 Epoch: 0378 loss_train: 0.6320 loss_val: 0.5709
Run:01 Epoch: 0379 loss_train: 0.6316 loss_val: 0.5707
Run:01 Epoch: 0380 loss_train: 0.6314 loss_val: 0.5705
Run:01 Epoch: 0381 loss_train: 0.6311 loss_val: 0.5703
Run:01 Epoch: 0382 loss_train: 0.6316 loss_val: 0.5701
Run:01 Epoch: 0383 loss_train: 0.6313 loss_val: 0.5699
Run:01 Epoch: 0384 loss_train: 0.6315 loss_val: 0.5698
Run:01 Epoch: 0385 loss_train: 0.6324 loss_val: 0.5697
Run:01 Epoch: 0386 loss_train: 0.6306 loss_val: 0.5697
Run:01 Epoch: 0387 loss_train: 0.6319 loss_val: 0.5697
Run:01 Epoch: 0388 loss_train: 0.6321 loss_val: 0.5696
Run:01 Epoch: 0389 loss_train: 0.6304 loss_val: 0.5695
Run:01 Epoch: 0390 loss_train: 0.6301 loss_val: 0.5693
Run:01 Epoch: 0391 loss_train: 0.6302 loss_val: 0.5692
Run:01 Epoch: 0392 loss_train: 0.6305 loss_val: 0.5691
Run:01 Epoch: 0393 loss_train: 0.6309 loss_val: 0.5690
Run:01 Epoch: 0394 loss_train: 0.6290 loss_val: 0.5688
Run:01 Epoch: 0395 loss_train: 0.6291 loss_val: 0.5687
Run:01 Epoch: 0396 loss_train: 0.6282 loss_val: 0.5685
Run:01 Epoch: 0397 loss_train: 0.6290 loss_val: 0.5683
Run:01 Epoch: 0398 loss_train: 0.6285 loss_val: 0.5682
Run:01 Epoch: 0399 loss_train: 0.6283 loss_val: 0.5681
Run:01 Epoch: 0400 loss_train: 0.6273 loss_val: 0.5680
test acc: 0.8064516129032259 test acc std 0.0


test micro f1: 0.8064516129032258 test macro f1 0.7789151604166054
